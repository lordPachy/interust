---
title: "reading_dataset"
format: html
editor: visual
---

# Preliminary code

## Importing packages

```{r}
library(ggplot2)
library(igraph)
library(tidyverse)
library(data.table)
library(anytime)
library(poweRlaw)
library(scales)
library(DBI)

#install.packages("RSQLite")
library(RSQLite)
#install.packages('goft')
library('goft')
#install.packages('useful')
library('useful')
#install.packages('ClusterR')
library(ClusterR)
#install.packages('FPDclustering')
library(FPDclustering)
#install.packages('jaccard')
#install.packages('bayesbio')
library('bayesbio')
#install.packages('gbm3')
library('gbm')
#install.packages('iGraphMatch')
library(iGraphMatch)
#install.packages('viridis')
library('viridis')
#install.packages('caret')
library('caret')
#install.packages('ranger')
library(ranger)
#remotes::install_github('mlampros/ClusterR', upgrade = 'always', dependencies = TRUE, repos = 'https://cloud.r-project.org/')
#install.packages('gmodels')
library(gmodels)
#install.packages('h2o')
library(h2o)
#install.packages("latex2exp")
library(latex2exp)
#install.packages('showtext')
library(showtext)
#install.packages('extrafont')
library('extrafont')
font_add_google("Raleway")
#font_import('raleway', '/home/pachy/Downloads/Raleway/static/Raleway-Regular.ttf')
showtext_auto()
```

## Importing data

```{r}
data_dir = '/home/pachy/Desktop/ACSAI/interust/data/public_id/dumps/postgresql/data/'
files = list.files('/home/pachy/Desktop/ACSAI/interust/data/public_id/dumps/postgresql/data/')

for (f in files){
  table = read.csv(paste(data_dir, f, sep = ''))
  assign(str_sub(f, 1, -5), setDT(table))
}
length(unique(followers[$follower_login))
length(unique(identities$identity))
```
## Importing DB through SQLite
```{r}
mydb = dbConnect(RSQLite::SQLite(), "/home/pachy/Desktop/ACSAI/interust/data/full/dumps/sqlite/rust_repos_2022_09_07_export.db")
dbGetQuery(mydb, 'SELECT * FROM followers')
```

## Importing pre-processed scripts
```{r}
contributions = setDT(read.csv("/home/pachy/Desktop/ACSAI/interust/data/full/examples/preprocessed/contributions.csv"))
dependencies = setDT(read.csv("/home/pachy/Desktop/ACSAI/interust/data/full/examples/preprocessed/dependencies.csv"))
usage = setDT(read.csv("/home/pachy/Desktop/ACSAI/interust/data/full/examples/preprocessed/usage.csv"))
```


# Cleaning data
## Contributions

```{r}
contributions
length(unique(contributions[, c(repo_id)]))
```


## Users and identities

```{r}
identities
```

```{r}
users
```

```{r}
#| label: Merging on attributes
users_raw = merge.data.table(x = identities, y = users, by.x = 'user_id', by.y = 'id')
users_raw = unique(users_raw)
users_raw
```

Identities and users are the same (bots are the same):

```{r}
all.equal(users_raw$is_bot.x, users_raw$is_bot.y)
```

The attribute identity_types is essentially useless

```{r}
identity_types
```

Let's filter out useless attributes from users_id

```{r}
users_raw = users_raw[, c('identity_type_id', 'is_bot.y', 'creation_identity_type_id'):=NULL]
users_raw
```


Testing datetime formats

```{r}
#print.POSIXct <- function(x,...)print(format(x,"%Y-%m-%d %H:%M:%S"))
x <- user_id_raw[id == 8716]$created_at
x
x <- as.POSIXct(x, tz=Sys.timezone())
x
mode(x)
y = copy(user_id_raw)
y[, created_at := anytime(created_at)]
y
min(y$created_at, na.rm = TRUE)
```

Let's do it for real

```{r}
#| label: Clean user table
users_clean = users_raw[, created_at := anytime(created_at)]
users_clean
```


## Followers

```{r}
followers_raw = copy(followers)
followers_raw
```

```{r}
followers_clean = followers_raw[, follower_identity_type_id := NULL]
followers_clean = followers_raw[, created_at := NULL]
followers_clean = followers_clean[!is.na(follower_id)]
frequency(follower_login)
```


## Stars
```{r}
stars_row = copy(stars)
stars_row
```
Let's check if login and identity_id coincide:
```{r}
users_clean[identity == '668b00f5737f27d7c7d0fa67a1241acf']
```
A bit of cleaning:
```{r}
stars_clean = stars_row[, starred_at := anytime(starred_at)]
stars_clean[, identity_type_id := NULL]
stars_clean = stars_clean[!is.na(identity_id)]
```

```{r}
all.equal(stars_clean$id, stars_clean$identity_id)
stars_clean
```

## Issues

```{r}
issues
issues_raw = copy(issues)
issues_raw
```
```{r}
issues_clean = issues_raw[, created_at := anytime(created_at)]
issues_clean = issues_clean[, closed_at := anytime(closed_at)]
issues_clean = issues_clean[, identity_type_id := NULL]
issues_clean = issues_clean[!is.na(author_id)]
```
## Issue reactions
```{r}
issue_reactions_raw = copy(issue_reactions)
issue_reactions_raw
```
```{r}
issue_reactions_clean = issue_reactions_raw[, created_at := anytime(created_at)]
issue_reactions_clean = issue_reactions_clean[, identity_type_id := NULL]
issue_reactions_clean = issue_reactions_clean[!is.na(author_id)]
```
## Issue comments
```{r}
issue_comments_raw = copy(issue_comments)
issue_comments_raw
```
```{r}
issue_comments_clean = issue_comments_raw[, created_at := anytime(created_at)]
issue_comments_clean = issue_comments_clean[, identity_type_id := NULL]
issue_comments_clean
issue_comments_clean = issue_comments_clean[!is.na(author_id)]
```
## Issue comment reactions
```{r}
issue_comment_reactions_raw = copy(issue_comment_reactions)
issue_comment_reactions_raw
```
```{r}
issue_comment_reactions_clean = issue_comment_reactions_raw[, created_at := anytime(created_at)]
issue_comment_reactions_clean = issue_comment_reactions_clean[, identity_type_id := NULL]
issue_comment_reactions_clean = issue_comment_reactions_clean[, !is.na(author_id)]
```
## Commits

```{r}
commits_raw = copy(commits)
commits_raw
```
```{r}
commits_clean = commits_raw[, ':='(local_created_at = NULL,
                                   time_offset = NULL,
                                   original_created_at = NULL,
                                   local_committed_at = NULL,
                                   time_offset_committed = NULL,
                                   original_committed_at = NULL)]
commits_clean = commits_clean[, ':='(created_at = anytime(created_at),
                                     committed_at = anytime(committed_at))]
commits_clean = commits_clean[, sha := NULL]
commits_clean
```

## Commit comments
```{r}
commit_comments_raw = copy(commit_comments)
commit_comments_raw
```

```{r}
commit_comments_clean = commit_comments_raw[, created_at := anytime(created_at)]
commit_comments_clean = commit_comments_clean[, identity_type_id := NULL]
commit_comments_clean
```
## Commit comment reactions
```{r}
commit_comment_reactions_raw = copy(commit_comment_reactions)
commit_comment_reactions_raw
```
```{r}
commit_comment_reactions_clean = commit_comment_reactions_raw[, created_at := anytime(created_at)]
commit_comment_reactions_clean = commit_comment_reactions_clean[, identity_type_id := NULL]
commit_comment_reactions_clean
```
## Pull requests

```{r}
pull_requests_raw = copy(pullrequests)
pull_requests_raw
```
```{r}
pull_requests_clean = pull_requests_raw[, ':='(pullrequest_title = NULL,
                                             created_at = anytime(created_at),
                                             merged_at = anytime(merged_at),
                                             closed_at = anytime(closed_at),
                                             identity_type_id = NULL,
                                             inserted_at = anytime(inserted_at))]
pull_requests_clean
```
## Pull request reactions
```{r}
pull_request_reactions_raw = copy(pullrequest_reactions)
pull_request_reactions_raw
```
```{r}
pull_request_reactions_clean = pull_request_reactions_raw[, ':='(identity_type_id = NULL,
                                                               created_at = anytime(created_at))]
pull_request_reactions_clean
```
## Pull request comments 

```{r}
pull_request_comments_raw = copy(pullrequest_comments)
pull_request_comments_raw
```
```{r}
pull_request_comments_clean = pull_request_comments_raw[, ':='(created_at = anytime(created_at),
                                                             identity_type_id = NULL)]
pull_request_comments_clean
```

## Pull request comment reactions

```{r}
pull_request_comment_reactions_raw = copy(pullrequest_comment_reactions)
pull_request_comment_reactions_raw
```
```{r}
pull_request_comment_reactions_clean = pull_request_comment_reactions_raw[, ':='(
                                                             created_at = anytime(created_at),
                                                             identity_type_id = NULL)]
pull_request_comment_reactions_clean
```
## Repositories
```{r}
repositories_raw = copy(repositories)
repositories_raw
```
```{r}
repositories_clean = repositories_raw[, ':='(created_at = anytime(created_at),
                                             latest_commit_time = anytime(latest_commit_time)
                                             )]
repositories_clean
```
## Watchers
```{r}
watchers_raw = copy(watchers)
watchers_raw
```
```{r}
watchers_clean = watchers_raw[, identity_type_id := NULL]
watchers_clean
```

## Forks
```{r}
forks_raw = copy(forks)
forks_raw
```
```{r}
forks_clean = forks_raw[, ':='(forking_repo_id = NULL,
                               forked_at = anytime(forked_at)
                               )]
forks_clean
```
## Organization memberships

```{r}
org_memberships_raw = copy(org_memberships)
org_memberships_raw
```
```{r}
org_memberships_clean = org_memberships_raw[, ':='(
  joined_at = NULL,
  left_at = NULL
)]
org_memberships_clean
```
## URLs
```{r}
urls_clean = copy(urls)
urls_clean
```

## Packages
```{r}
packages_raw = copy(packages)
packages_raw
```

```{r}
packages_clean = packages_raw[, created_at := anytime(created_at)]
packages_clean
```
## Package versions
```{r}
package_versions_raw = copy(package_versions)
package_versions_raw
```
```{r}
package_versions_clean = package_versions_raw[, created_at := anytime(created_at)]
package_versions_clean
```

## Package version downloads

```{r}
# package_version_downloads_clean = copy(package_version_downloads)
# package_version_downloads_clean = package_version_downloads_clean[, downloaded_at := anytime(downloaded_at)]
# package_version_downloads_clean[downloaded_at != anytime("2022-09-07")]

```

```{r}
#| label: Testing identity intersections

login_sc = stars_clean[, 'login']
login_sc = login_sc[!duplicated(login_sc)]
login_sc
login_id = users_clean[, 'login']
login_id
login_id = login_id[!duplicated(login_id)]
login_id
all(as.vector(login_id) %in% (login_sc))
intersect(login_id, login_sc)
```


# Aggregating data

## Followers
### Number of followers

```{r}
followers_clean
n_followers = followers_clean[, .N, by = followee_id]
setnames(n_followers, c('N', 'followee_id'), c('n_followers', 'id'))
n_followers = n_followers[!is.na(id)]
n_followers
```
### Number of following

```{r}
n_following = followers_clean[, .N, by = follower_id]
setnames(n_following, c('follower_id', 'N'), c('id', 'n_following'))
n_following = remove_missing(n_following)
n_following
```

## Users on repositories
We can try to match users to repositories through commits.
```{r}
users_repos = unique(commits_clean[, .(author_id, repo_id)])
users_repos = users_repos[!is.na(author_id)]

#Alternative version with preprocessed files
cols_useful = c('repo_id', 'user_id')
users_repos_pre = unique(contributions[, ..cols_useful])
setnames(users_repos_pre, 'user_id', 'author_id')
users_repos = users_repos[!is.na(author_id)]


users_repos_pre
```


## Stars
### Put
```{r}
stars_clean
n_stars_put = stars_clean[, .N, by = identity_id]
setnames(n_stars_put, c('N', 'identity_id'), c('n_stars_put', 'id'))
n_stars_put = remove_missing(n_stars_put)


#Alternative version with preprocessed files
n_stars_put
col = c('login', 'identity_id')
stars_clean[, ..col]
n_stars_put[]
is_true = merge.data.table(
  x = n_stars_put,
  y = n_following,
  by = 'login'
)
is_really_true = merge.data.table(
  x = is_true,
  y = users_clean,
  by = 'login',
)

is_really_true
```

### Received
```{r}
stars_clean
stars_per_repo = stars_clean[, .N, by = repo_id]
stars_per_repo
users_repos
stars_per_user = merge.data.table(
  x = users_repos,
  y = stars_per_repo,
  by.x = 'repo_id',
  by.y = 'repo_id'
)
stars_per_user
n_stars_received = stars_per_user[, sum(N), by = author_login]
setnames(n_stars_received, c('author_id', 'V1'), c('id', 'n_stars_received'))
n_stars_received = n_stars_received[!is.na(id)]
n_stars_received
```

## Issues

### Posted by users

```{r}
issues_clean
n_issues_posted = issues_clean[, .N, by = author_id]
setnames(n_issues_posted, c('author_id', 'N'), c('id', 'n_issues_posted'))
n_issues_posted = n_issues_posted[!is.na(id)]
n_issues_posted
```
### Per repository
```{r}
n_issues_per_repo = issues_clean[, .N, by = repo_id]
setnames(n_issues_per_repo, 'N', 'n_issues_per_repo')
n_issues_per_repo
```
## Issue reactions
### Put
```{r}
n_issue_reactions_put= issue_reactions_clean[, .N, by = author_id]
setnames(n_issue_reactions_put, c('author_id', 'N'), c('id', 'n_issue_reactions_put'))
n_issue_reactions_put = n_issue_reactions_put[!is.na(id)]
n_issue_reactions_put
```
### Received
```{r}
#First, we count all the issues that have more than 0 reactions
reactions_per_issue = merge.data.table(
  x = issues_clean,
  y = issue_reactions_clean, 
  by.x = c('repo_id', 'issue_number'),
  by.y = c('repo_id', 'issue_number')
)
n_issue_reactions_received = reactions_per_issue[, .N, by = author_id.x]
n_issue_reactions_received
setnames(n_issue_reactions_received, c('author_id.x', 'N'), c('id', 'n_issue_reactions_received'))
n_issue_reactions_received = n_issue_reactions_received[!is.na(id)]
n_issue_reactions_received
```
## Issue comments
### Posted
```{r}
n_issue_comments_posted= issue_comments_clean[, .N, by = author_id]
setnames(n_issue_comments_posted, c('author_id', 'N'), c('id', 'n_issue_comments_posted'))
n_issue_comments_posted = n_issue_comments_posted[!is.na(id)]
n_issue_comments_posted
```
### Received
```{r}
comments_per_issue = merge.data.table(
  x = issues_clean,
  y = issue_comments_clean, 
  by.x = c('repo_id', 'issue_number'),
  by.y = c('repo_id', 'issue_number'),
)
n_issue_comments_received = comments_per_issue[, .N, by = author_id.x]
setnames(n_issue_comments_received, c('author_id.x', 'N'), c('id', 'n_issue_comments_received'))
n_issue_comments_received = n_issue_comments_received[!is.na(id)]
n_issue_comments_received
```
## Issue comment reactions
### Put
```{r}
n_issue_comment_reactions_put= issue_comment_reactions_clean[, .N, by = author_id]
setnames(n_issue_comment_reactions_put, c('author_id', 'N'), c('id', 'n_issue_comment_reactions_put'))
n_issue_comment_reactions_put = n_issue_comment_reactions_put[!is.na(id)]
n_issue_comment_reactions_put
```
### Received
```{r}
reactions_per_issue_comment = merge.data.table(
  x = issue_comments_clean,
  y = issue_comment_reactions_clean, 
  by.x = c('repo_id', 'issue_number', 'comment_id'),
  by.y = c('repo_id', 'issue_number', 'comment_id')
)
reactions_per_issue_comment
n_issue_comment_reactions_received = reactions_per_issue_comment[, .N, by = author_id.x]
n_issue_comment_reactions_received
setnames(n_issue_comment_reactions_received, c('author_id.x', 'N'), c('id', 'n_issue_comment_reactions_received'))
n_issue_comment_reactions_received = n_issue_comment_reactions_received[!is.na(id)]
n_issue_comment_reactions_received
```

## Commits
```{r}
n_commits = commits_clean[, .N, by = author_id]
setnames(n_commits, c('author_id', 'N'), c('id', 'n_commits'))
n_commits = n_commits[!is.na(id)]

#Alternative version with preprocessed files
contributions
n_commits_pre = contributions[, sum(nb_commits), by = user_id]
setnames(n_commits_pre, c('user_id', 'V1'), c('id', 'n_commits'))
n_commits_pre = n_commits_pre[!is.na(id)]

n_commits
```
## Commit comments
### Posted
```{r}
n_commit_comments_posted= commit_comments_clean[, .N, by = author_id]
setnames(n_commit_comments_posted, c('author_id', 'N'), c('id', 'n_commit_comments_posted'))
n_commit_comments_posted = n_commit_comments_posted[!is.na(id)]
n_commit_comments_posted
```
### Received
```{r}
comments_per_commit = merge.data.table(
  x = commits_clean,
  y = commit_comments_clean, 
  by.x = c('repo_id', 'id'),
  by.y = c('repo_id', 'commit_id'),
  all.x = T
)
comments_per_commit 
comments_per_commit
n_commit_comments_received = comments_per_commit[, .N, by = author_id.x]
setnames(n_commit_comments_received, c('author_id.x', 'N'), c('id', 'n_commit_comments_received'))
n_commit_comments_received = n_commit_comments_received[!is.na(id)]
n_commit_comments_received
```
## Commit comment reactions
### Put
```{r}
n_commit_comment_reactions_put= commit_comment_reactions_clean[, .N, by = author_id]
setnames(n_commit_comment_reactions_put, c('author_id', 'N'), c('id', 'n_commit_comment_reactions_put'))
n_commit_comment_reactions_put = n_commit_comment_reactions_put[!is.na(id)]
n_commit_comment_reactions_put
```
### Received
```{r}
reaction_per_commit_comment = merge.data.table(
  x = commit_comments_clean,
  y = commit_comment_reactions_clean, 
  by.x = c('repo_id', 'commit_id', 'comment_id'),
  by.y = c('repo_id', 'commit_id', 'comment_id')
)
reaction_per_commit_comment
n_commit_comment_reactions_received = reaction_per_commit_comment[, .N, by = author_id.x]
setnames(n_commit_comment_reactions_received, c('author_id.x', 'N'), c('id', 'n_commit_comment_reactions_received'))
n_commit_comment_reactions_received = n_commit_comment_reactions_received[!is.na(id)]
n_commit_comment_reactions_received
```
## Pull requests
```{r}
pull_requests_clean
n_pull_requests = pull_requests_clean[, .N, by = author_id]
setnames(n_pull_requests, c('author_id', 'N'), c('id', 'n_pull_requests'))
n_pull_requests = n_pull_requests[!is.na(id)]
n_pull_requests
```
## Pull request reactions
```{r}
pull_request_reactions_clean
```
### Put
```{r}
n_pull_request_reactions_put = pull_request_reactions_clean[, .N, by = author_id]
setnames(n_pull_request_reactions_put, c('author_id', 'N'), c('id', 'n_pull_request_reactions_put'))
n_pull_request_reactions_put = n_pull_request_reactions_put[!is.na(id)]
n_pull_request_reactions_put
```
### Received
```{r}
reaction_per_pull_request = merge.data.table(
  x = pull_requests_clean,
  y = pull_request_reactions_clean, 
  by.x = c('repo_id', 'pullrequest_number'),
  by.y = c('repo_id', 'pullrequest_number')
)
reaction_per_pull_request
n_pull_request_reactions_received = reaction_per_pull_request[, .N, by = author_id.x]
setnames(n_pull_request_reactions_received, c('author_id.x', 'N'), c('id', 'n_pull_request_reactions_received'))
n_pull_request_reactions_received = n_pull_request_reactions_received[!is.na(id)]
n_pull_request_reactions_received
```
## Pull request comments
```{r}
pull_request_comments_clean
```
### Posted
```{r}
n_pull_request_comments_posted = pull_request_comments_clean[, .N, by = author_id]
setnames(n_pull_request_comments_posted, c('author_id', 'N'), c('id', 'n_pull_request_comments_posted'))
n_pull_request_comments_posted = n_pull_request_comments_posted[!is.na(id)]
n_pull_request_comments_posted
```
### Received
```{r}
comments_per_pull_request = merge.data.table(
  x = pull_requests_clean,
  y = pull_request_comments_clean, 
  by.x = c('repo_id', 'pullrequest_number'),
  by.y = c('repo_id', 'pullrequest_number')
)
comments_per_pull_request
n_pull_request_comments_received = comments_per_pull_request[, .N, by = author_id.x]
setnames(n_pull_request_comments_received, c('author_id.x', 'N'), c('id', 'n_pull_request_comments_received'))
n_pull_request_comments_received = n_pull_request_comments_received[!is.na(id)]
n_pull_request_comments_received
```
## Pull request comment reactions
```{r}
pull_request_comment_reactions_clean
```
### Put
```{r}
n_pull_request_comment_reactions_put = pull_request_comment_reactions_clean[, .N, by = author_id]
setnames(n_pull_request_comment_reactions_put, c('author_id', 'N'), c('id', 'n_pull_request_comment_reactions_put'))
n_pull_request_comment_reactions_put = n_pull_request_comment_reactions_put[!is.na(id)]
n_pull_request_comment_reactions_put
```
### Received
```{r}
comment_reactions_per_pull_request = merge.data.table(
  x = pull_request_comments_clean,
  y = pull_request_comment_reactions_clean, 
  by.x = c('repo_id', 'pullrequest_number', 'comment_id'),
  by.y = c('repo_id', 'pullrequest_number', 'comment_id')
)
comment_reactions_per_pull_request
n_pull_request_comment_reactions_received = comment_reactions_per_pull_request[, .N, by = author_id.x]
setnames(n_pull_request_comment_reactions_received, c('author_id.x', 'N'), c('id', 'n_pull_request_comment_reactions_received'))
n_pull_request_comment_reactions_received = n_pull_request_comment_reactions_received[!is.na(id)]
n_pull_request_comment_reactions_received
```
## Watchers
```{r}
watchers_clean
```
### Watching
```{r}
n_repo_watched_per_user = watchers_clean[, .N, by = identity_id]
setnames(n_repo_watched_per_user, c('identity_id', 'N'), c('id', 'n_repo_watched_per_user'))
n_repo_watched_per_user = n_repo_watched_per_user[!is.na(id)]
n_repo_watched_per_user
```

### Watched
```{r}
n_being_watched_from = merge.data.table(
  x = users_repos,
  y = watchers_clean, 
  by.x = c('repo_id'),
  by.y = c('repo_id'),
  allow.cartesian = T
)
n_being_watched_from
n_being_watched_from = n_being_watched_from[, .N, by = author_id]
setnames(n_being_watched_from, c('author_id', 'N'), c('id', 'n_being_watched_from'))
n_being_watched_from = n_being_watched_from[!is.na(id)]
n_being_watched_from
```
## Forks
```{r}
forks_clean
```
## Organization memberships
```{r}
org_memberships_clean
sources
```
## Repo downloads
```{r}
# usage
# 
# repo_downloads = usage[, sum(downloads), by = repo_id]
# setnames(repo_downloads, 'V1', 'n_downloads')
# repo_downloads = repo_downloads[!is.na(repo_id)]
# users_repos_pre
# users_downloads = merge.data.table(
#   x = users_repos_pre,
#   y = repo_downloads,
#   by = 'repo_id'
# )
# n_downloads = users_downloads[, sum(n_downloads), by = author_id]
# setnames(n_downloads, c('author_id', 'V1'), c('id', 'n_downloads'))
# n_downloads = n_downloads[!is.na(id)]
# repo_downloads
```

```{r}
# versions_per_package = merge.data.table(
#   x = packages_clean,
#   y = package_versions_clean,
#   by.x = c('id'),
#   by.y = c('package_id'),
# )
# versions_per_package
# downloads_per_package = merge.data.table(
#   x = package_versions_clean,
#   y = package_version_downloads_clean,
#   by.x = 'id',
#   by.y = 'package_version'
# )
# downloads_per_package
# downloads_per_repo = downloads_per_package[, .N, by = repo_id]
# n_being_watched_from = n_being_watched_from[, .N, by = author_id]
# setnames(n_being_watched_from, c('author_id', 'N'), c('id', 'n_being_watched_from'))
# n_being_watched_from = remove_missing(n_being_watched_from)
# n_being_watched_from
```
## Final score per user
```{r}
users_clean
setnames(users_clean, 'identity', 'login')
users_clean
n_followers
n_following
```

```{r}
n_followers
users_with_scores = merge.data.table(
  x = users_clean,
  y = n_followers,
  by.x = 'id',
  by.y = 'id',
  all.x = T
)
users_with_scores[is.na(n_followers), n_followers := 0]

users_with_scores[n_followers != 0]

users_with_scores = merge.data.table(
  x = users_with_scores,
  y = n_following,
  by.x = 'id',
  by.y = 'id',
  all.x = T
)
users_with_scores[is.na(n_following), n_following := 0]

users_with_scores = merge.data.table(
  x = users_with_scores,
  y = n_stars_put,
  by.x = 'id',
  by.y = 'id',
  all.x = T
)
users_with_scores[is.na(n_stars_put), n_stars_put := 0]

users_with_scores = merge.data.table(
  x = users_with_scores,
  y = n_stars_received,
  by.x = 'id',
  by.y = 'id',
  all.x = T
)
users_with_scores[is.na(n_stars_received), n_stars_received := 0]

users_with_scores = merge.data.table(
  x = users_with_scores,
  y = n_issues_posted,
  by.x = 'id',
  by.y = 'id',
  all.x = T
)
users_with_scores[is.na(n_issues_posted), n_issues_posted := 0]

users_with_scores = merge.data.table(
  x = users_with_scores,
  y = n_issue_reactions_put,
  by.x = 'id',
  by.y = 'id',
  all.x = T
)

users_with_scores[is.na(n_issue_reactions_put), n_issue_reactions_put := 0]


users_with_scores = merge.data.table(
  x = users_with_scores,
  y = n_issue_reactions_received,
  by.x = 'id',
  by.y = 'id', 
  all.x = T
)

users_with_scores[is.na(n_issue_reactions_received), n_issue_reactions_received := 0]


users_with_scores = merge.data.table(
  x = users_with_scores,
  y = n_issue_comments_posted,
  by.x = 'id',
  by.y = 'id',
  all.x = T
)
users_with_scores[is.na(n_issue_comments_posted), n_issue_comments_posted := 0]

users_with_scores = merge.data.table(
  x = users_with_scores,
  y = n_issue_comments_received,
  by.x = 'id',
  by.y = 'id',
  all.x = T
)

users_with_scores[is.na(n_issue_comments_received), n_issue_comments_received := 0]

users_with_scores = merge.data.table(
  x = users_with_scores,
  y = n_issue_comment_reactions_put,
  by.x = 'id',
  by.y = 'id',
  all.x = T
)
users_with_scores[is.na(n_issue_comment_reactions_put), n_issue_comment_reactions_put := 0]

users_with_scores = merge.data.table(
  x = users_with_scores,
  y = n_issue_comment_reactions_received,
  by.x = 'id',
  by.y = 'id',
  all.x = T
)

users_with_scores[is.na(n_issue_comment_reactions_received), n_issue_comment_reactions_received := 0]

users_with_scores
users_with_scores = merge.data.table(
  x = users_with_scores,
  y = n_commits,
  by.x = 'id',
  by.y = 'id',
  all.x = T
)

users_with_scores[is.na(n_commits), n_commits := 0]

users_with_scores = merge.data.table(
  x = users_with_scores,
  y = n_commit_comments_posted,
  by.x = 'id',
  by.y = 'id',
  all.x = T
)

users_with_scores[is.na(n_commit_comments_posted), n_commit_comments_posted := 0]


users_with_scores = merge.data.table(
  x = users_with_scores,
  y = n_commit_comments_received,
  by.x = 'id',
  by.y = 'id',
  all.x = T
)

users_with_scores[is.na(n_commit_comments_received), n_commit_comments_received := 0]

users_with_scores = merge.data.table(
  x = users_with_scores,
  y = n_commit_comment_reactions_put,
  by.x = 'id',
  by.y = 'id',
  all.x = T
)

users_with_scores[is.na(n_commit_comment_reactions_put), n_commit_comment_reactions_put := 0]
users_with_scores = merge.data.table(
  x = users_with_scores,
  y = n_commit_comment_reactions_received,
  by.x = 'id',
  by.y = 'id',
  all.x = T
)

users_with_scores[is.na(n_commit_comment_reactions_received), n_commit_comment_reactions_received := 0]

users_with_scores = merge.data.table(
  x = users_with_scores,
  y = n_pull_requests,
  by.x = 'id',
  by.y = 'id',
  all.x = T
)

users_with_scores[is.na(n_pull_requests), n_pull_requests := 0]
users_with_scores = merge.data.table(
  x = users_with_scores,
  y = n_pull_request_reactions_put,
  by.x = 'id',
  by.y = 'id',
  all.x = T
)

users_with_scores[is.na(n_pull_request_reactions_put), n_pull_request_reactions_put := 0]
users_with_scores = merge.data.table(
  x = users_with_scores,
  y = n_pull_request_reactions_received,
  by.x = 'id',
  by.y = 'id',
  all.x = T
)

users_with_scores[is.na(n_pull_request_reactions_received), n_pull_request_reactions_received := 0]
```

```{r}
#| label: dai
users_with_scores = merge.data.table(
  x = users_with_scores,
  y = n_pull_request_comments_posted,
  by.x = 'id',
  by.y = 'id',
  all.x = T
)

users_with_scores[is.na(n_pull_request_comments_posted), n_pull_request_comments_posted := 0]
users_with_scores = merge.data.table(
  x = users_with_scores,
  y = n_pull_request_comments_received,
  by.x = 'id',
  by.y = 'id',
  all.x = T
)

users_with_scores[is.na(n_pull_request_comments_received), n_pull_request_comments_received := 0]
users_with_scores = merge.data.table(
  x = users_with_scores,
  y = n_pull_request_comment_reactions_put,
  by.x = 'id',
  by.y = 'id',
  all.x = T
)

users_with_scores[is.na(n_pull_request_comment_reactions_put), n_pull_request_comment_reactions_put := 0]
users_with_scores = merge.data.table(
  x = users_with_scores,
  y = n_pull_request_comment_reactions_received,
  by.x = 'id',
  by.y = 'id',
  all.x = T
)

users_with_scores[is.na(n_pull_request_comment_reactions_received), n_pull_request_comment_reactions_received := 0]
users_with_scores = merge.data.table(
  x = users_with_scores,
  y = n_repo_watched_per_user,
  by.x = 'id',
  by.y = 'id',
  all.x = T
)

users_with_scores[is.na(n_repo_watched_per_user), n_repo_watched_per_user := 0]
users_with_scores = merge.data.table(
  x = users_with_scores,
  y = n_being_watched_from,
  by.x = 'id',
  by.y = 'id',
  all.x = T
)

users_with_scores[is.na(n_being_watched_from), n_being_watched_from := 0]
```

```{r}
#| label: creating aggregate scores

users_aggregate = users_with_scores[, ':='(social_popularity =
                                          n_followers +
                                          n_stars_received +
#                                          n_issue_reactions_received,
#                                          n_issue_comment_reactions_received,
#                                          n_commit_comment_reactions_received,
#                                          n_pull_request_reactions_received,
#                                          n_pull_request_comment_reactions_received,
                                          n_being_watched_from,
                                          social_activity =
                                          n_following +
                                          n_stars_put +
#                                          n_issue_reactions_put,
#                                          n_issue_comment_reactions_put,
#                                          n_commit_comment_reactions_put,
#                                          n_pull_request_reactions_put,
#                                          n_pull_request_comment_reactions_put,
                                          n_repo_watched_per_user,
#                                      working_popularity =
#                                        sum(n_issue_comments_received,
#                                            n_commit_comments_received,
#                                            n_pull_request_comments_received),
                                      working_activity = 
                                            n_issues_posted +
                                           n_issue_comments_posted +
                                           n_commit_comments_posted +
                                            n_pull_request_comments_posted +
                                            n_commits +
                                            n_pull_requests)]


users_aggregate = users_aggregate[(social_activity > 10) &(working_activity > 10) & (social_popularity > 10)]
users_aggregate_log = copy(users_aggregate)
users_aggregate_log = users_aggregate_log[, ':='(
  working_activity = log(working_activity, base = 10),
  social_activity = log(working_activity, base = 10),
  social_popularity = log(social_popularity, base = 10))
  ]
users_aggregate_log = users_aggregate_log[working_activity <4 & social_popularity < 3
  ]
users_aggregate_log[, 'social_activity']
```


# Exploring distributions
##Basic info
```{r}
summary(users_aggregate)
```

## Downloads per user: what is happening
```{r}
ggplot(users_with_scores) + geom_density(aes(n_downloads, ..scaled..)) + scale_x_log10()
```


## Regression on various scores
```{r}
lm = lm(formula = n_commit_comments_posted ~ 
#          n_following
#        + n_stars_put 
#        + n_stars_received 
#        + n_issues_posted 
#        + n_issue_reactions_put 
#        + n_issue_reactions_received 
#        + n_issue_comments_posted 
#        + n_issue_comments_received
#        + n_issue_comment_reactions_put 
#        + n_issue_comment_reactions_received  
#        + n_commit_comments_posted 
#        + n_commit_comments_received, 
#        + n_commit_comment_reactions_put 
#        + n_commit_comment_reactions_received 
          n_commits, 
#        + n_pull_request_reactions_put 
#        + n_pull_request_reactions_received 
#        + n_pull_request_comments_posted 
#        + n_pull_request_comments_received 
#        + n_pull_request_comment_reactions_put 
#        + n_pull_request_comment_reactions_received 
#        + n_being_watched_from, 
#        + n_repo_watched_per_user,
        data = users_with_scores)
summary(lm)
```
```{r}
lm = lm(formula = social_popularity ~ working_activity, data = users_aggregate_log)
summary(lm)
```
# Most working active users cannot be ignored


```{r}
scale_factor = 'identity'
ggplot(users_aggregate_log) + geom_point(aes(working_activity, social_popularity))+ scale_x_continuous(transform = scale_factor) + scale_y_continuous(transform = scale_factor)
```
But let's count how many belong to each group

```{r}
count(users_aggregate_log[social_popularity > 3 & social_popularity < 3.5 & working_activity < 4 & working_activity > 0])
```


## Clustering analysis
```{r}
#| label: Useful columns
useful_columns = c(
#          'n_following',
#         'n_stars_put',
#         'n_stars_received',
#         'n_issues_posted',
#          'n_issue_reactions_put',
         # 'n_issue_reactions_received',
#         'n_issue_comments_posted',
         #'n_issue_comments_received',
#          'n_issue_comment_reactions_put',
         # 'n_issue_comment_reactions_received',
#         'n_commit_comments_posted',
         #'n_commit_comments_received',
#          'n_commit_comment_reactions_put',
         # 'n_commit_comment_reactions_received',
#         'n_commits',
         # 'n_pull_request_reactions_put',
#          'n_pull_request_reactions_received',
#         'n_pull_request_comments_posted',
         #'n_pull_request_comments_received',
#         'n_pull_request_comment_reactions_put',
         #'n_pull_request_comment_reactions_received',
#         'n_being_watched_from',
#         'n_repo_watched_per_user'
#            'social_activity',
            'working_activity',
            'social_popularity')
users_useful_features = users_aggregate_log[, ..useful_columns]
users_useful_features = users_useful_features
users_useful_features
```

```{r}
#| label: K-Means clustering
set.seed(42)
n = 4
km = kmeans(users_useful_features, n, iter.max = 100L)
#km$centers[3*n]
#df = data.frame(social_activity = km$centers[1:n], working_activity = km$centers[(n+1):(2*n)], social_popularity = km$centers[(2*n+1):(3*n)])
#df = df |> mutate(id = row_number())
length(km$cluster)
#ggplot(df, aes(working_activity, social_popularity)) + geom_point()
km
```
```{r}
#| label: Clustered users

km$cluster
users_aggregate_cluster = users_aggregate_log[, 'cluster' := km$cluster]
```

## A graph of clusters

```{r}
clusters_plot = ggplot(users_aggregate_cluster, aes(working_activity, social_popularity, colour = factor(cluster))) + geom_point() +
  labs(title = "Users by working activity and social popularity",
       x = "Working activity score",
       y = "Social popularity score"
       ) +
  scale_x_continuous(breaks = 1:4, labels = c("1", "10", "100", "1000")) + 
  scale_y_continuous(breaks = 1:4, labels = c("1", "10", "100", "1000")) + 
  scale_color_manual(values = c("#44ac9e", "#a83232", "#9ca832", "#ee7027"),
                       breaks = 4:1,
                       labels = c('Top users', 'Newbies', 'Ants', 'Businessmen')
                       ) +
  theme(axis.line = element_line(arrow = arrow(angle = 20, length = unit(.10,"inches"),type = "closed")),
      axis.title.x = element_text(face = 'plain', size = 40),
      axis.title.y = element_text(face = 'plain', size = 40),
     text = element_text('Raleway', face = 'bold', size = 50),
     panel.background = element_rect(fill = "gray92", colour = NA),
     #title.text = element_text('NewCenturySchoolBook') + 
  )  +
  labs(colour = "Cluster")
```
 '#44ac9e', high = '#ee7027'

```{r}
ggsave(
  '/home/pachy/Desktop/ACSAI/interust/graphs/clusterplot.png',
  plot = clusters_plot,
  device = NULL,
  path = NULL,
  scale = 1,
  width = 2100,
  height = 1296,
  units = c("px"),
  dpi = 300,
  limitsize = TRUE,
  bg = NULL,
  create.dir = FALSE
)
```




```{r}
#| label: Analysis of popular, non-working people

users_aggregate_log[km$cluster == 12 | km$cluster == 14]$n_being_watched_from
summary(users_aggregate_log[km$cluster == 12 | km$cluster == 14])
ggplot(users_aggregate, aes(n_being_watched_from)) + geom_density() + scale_x_log10()
```

```{r}
#| label: Lucky vs population graph comparisons: n_being_watched_from

#choose the parameters that need to be analyzed
col_of_interest = 'n_being_watched_from'

#observe them
users_aggregate_log[, ..col_of_interest]

#create the vectors of interest
x = users_aggregate_log[[col_of_interest]]
#x
y = users_aggregate_log[km$cluster == 1 | km$cluster == 9 | km$cluster == 14]
y = y[[col_of_interest]]
#y

#count how many observations are needed to complete the table
n_observations = length(x)
#n_observations
n_partial_observations = length(y)
#n_observations - n_partial_observations

#compose the data frames
comparison_lucky = data.frame(x = x, 
                              y = c(y, rep(NA, n_observations - n_partial_observations)))

#plot
#comparison_lucky
ggplot(comparison_lucky) +
  scale_color_manual(
    labels = c("General distribution", "Lucky people distribution"),
    values = c("red", "blue")
    )+
  geom_density(aes(x, colour = 'red', ..scaled..)) + 
  geom_density(aes(y, colour = 'blue', ..scaled..)) + 
  scale_x_log10() + 
  xlab(col_of_interest) + 
  ylab('frequency')
```


```{r}
#| label: Hard-working vs population graph comparisons: 

#choose the parameters that need to be analyzed
col_of_interest = 'n_being_watched_from'

#observe them
users_aggregate_log[, ..col_of_interest]

#create the vectors of interest
x = users_aggregate_log[[col_of_interest]]
#x
y = users_aggregate_log[km$cluster == 16]
y = y[[col_of_interest]]
#y

#count how many observations are needed to complete the table
n_observations = length(x)
#n_observations
n_partial_observations = length(y)
#n_observations - n_partial_observations

#compose the data frames
comparison_hard_working = data.frame(x = x, 
                              y = c(y, rep(NA, n_observations - n_partial_observations)))

#plot
#comparison_hard_working
ggplot(comparison_hard_working) +
  scale_color_manual(
    labels = c("General distribution", "Hard-working people distribution"),
    values = c("red", "blue")
    )+
  geom_density(aes(x, colour = 'red', ..scaled..)) + 
  geom_density(aes(y, colour = 'blue', ..scaled..)) + 
  scale_x_log10() + 
  xlab(col_of_interest) + 
  ylab('frequency')
```
```{r}
#| label: Jaccard similarity on these groups

jaccardSets(comparison_hard_working$x, comparison_hard_working$y)
```



```{r}
#| label: Hierarchical clustering

hclust_users = hclust(d=dist(users_useful_features[sample(.N, 50000)])) 
tree = cutree(hclust_users, k = 5)
plot(tree)
```
```{r}
#| label: Probability clustering

res = GPDC(data = users_useful_features[sample(.N, 5000)], k = 3)
table(res$n_commits)
plot(res)
```


## Users

Let's see how many bots we have

```{r}
ggplot(user_id_raw, aes(x = is_bot)) + geom_bar()
```

Not so many.

Let's see how many creation dates we have

```{r}
created_or_generated = copy(user_id_raw)
created_or_generated[, birthdate_unknown := created_at=='']
created_or_generated
ggplot(created_or_generated, aes(x = birthdate_unknown)) + geom_bar()
```

```{r}
ggplot(user_id_raw, aes(x = year(created_at))) + geom_bar()
```


## Stars
Let's see how many stars do they put:
```{r}
user_stars_nobot = user_stars[is_bot == F]
user_stars_nobot = user_stars[, .N, by = identity]
user_stars_nobot
ggplot(user_stars_nobot[N<2000], aes(x = N)) + geom_density() + scale_y_continuous(trans = 'log10') + scale_x_continuous(trans = 'log10')
```
Let's change the visualization:
```{r}
t = new_transform(name = 'x', function(x) log(x, base = exp(1)), function(x) exp(x))
ggplot(user_stars_nobot[N<2000], aes(x = N)) + geom_density() + scale_y_continuous(trans = t)
```

Let's try to fit a power law:
```{r}
#| label: Power law fitting
user_stars_nobot
m = displ$new(user_stars_nobot$N)
est = estimate_xmin(m)
est
m$setXmin(est)
bootstrap_p(m, no_of_sims = 10, threads = 4)
```

### Old data

```{r}
follower_degree = followers_aggr[, .N, by = followee_id]
#follower_degree = follower_degree[N > 486]
ggplot(follower_degree, aes(x = N)) + geom_bar(width = 1000) + scale_y_continuous(trans = 'log10')
```

Let's check if it fits a power law distribution:

```{r}
#| label: Power law fitting
m_m = displ$new(follower_degree$N)
est = estimate_xmin(m_m)
est
```

```{r}
#| label: Power law fitting
m_m$setXmin(est)
plot(m_m)
lines(m_m, col = 2)
```

Let's see if it fits:
```{r}
#| label: Power law fitting
parallel::detectCores()
bs = bootstrap_p(m_m, no_of_sims = 10, threads = 2)
bs
```
## Stars
Let's see how many stars do they put:
```{r}
user_stars_nobot = user_stars[is_bot == F]
user_stars_nobot = user_stars[, .N, by = identity]
user_stars_nobot
ggplot(user_stars_nobot[N<2000], aes(x = N)) + geom_density() + scale_y_continuous(trans = 'log10') + scale_x_continuous(trans = 'log10')
```
Let's change the visualization:
```{r}
t = new_transform(name = 'x', function(x) log(x, base = exp(1)), function(x) exp(x))
ggplot(user_stars_nobot[N<2000], aes(x = N)) + geom_density() + scale_y_continuous(trans = t)
```

Let's try to fit a power law:
```{r}
#| label: Power law fitting
user_stars_nobot
m = displ$new(user_stars_nobot$N)
est = estimate_xmin(m)
est
m$setXmin(est)
bootstrap_p(m, no_of_sims = 10, threads = 4)
```

## Log normal followers' distribution
```{r}
#| label: Getting the values
m_m = dislnorm$new(users_aggregate[n_followers>55]$n_followers)
est_xmin = estimate_xmin(m_m, distance = 'reweight')
est_xmin
m_m$setXmin(est_xmin)
est_pars = estimate_pars(m_m)
m_m$setPars(est_pars)
```

```{r}
#| label: Basic plots
plot(m_m)
lines(m_m, col = 2)
```

```{r}
#| label: Simulation
bootstrap_p(m_m, no_of_sims = 30, threads = 4, distance = 'reweight')
```

```{r}
#| label: Plotting theoretical vs real followers distribution

distr = dist_pdf(m_m, q = NULL, log = FALSE)
points = dist_rand(m_m, n = 20833)
points
length(users_aggregate[n_followers>55]$n_followers)
df = data.frame(theoretical = points, real = users_aggregate[n_followers>55]$n_followers)
ggplot(data = df) +
  geom_density(mapping = aes(x = theoretical, colour = 'red')) +
  scale_color_manual(
    labels = c("Real distribution", "Theoretical distribution"),
    values = c("blue", "red")
    ) + 
  geom_density(mapping = aes(x = real, colour = 'blue')) +
  scale_x_continuous(trans = 'log10') +
  xlab('N of followers')
```
```{r}
#| label: A different test for log normality
data = sample(users_aggregate[n_followers>55]$n_followers, 5000, replace = F)
data
goft::weibull_test(data)
```

```{r}
users_aggregate[n_followers>55]$n_followers
followers_logscale = log(users_aggregate[n_followers>0]$n_followers, base = 20)
followers_loglogscale = log(users_aggregate[n_followers>0]$n_followers, base = 5)
followers_loglogscale
followers_df = data.frame(x = followers_loglogscale)
ggplot(followers_df, aes(x)) + geom_density()
```

```{r}
followers_loglogscale = log(log(users_aggregate[n_followers>10]$n_followers))
shapiro.test(sample(followers_loglogscale, 5000, replace = T))
```

```{r}
#| label: how it should go

normal_data <- rnorm(200)

#perform kolmogorov-smirnov test
ks.test(normal_data, 'pnorm')
```

## Exponential followers' distribution
```{r}
m_m = disexp$new(users_aggregate[n_followers>0]$n_followers)
est_xmin = estimate_xmin(m_m)
est_xmin
m_m$setXmin(est_xmin)
#est_pars = estimate_pars(m_m)
#m_m$setPars(est_pars)
```

```{r}
plot(m_m)
lines(m_m, col = 2)
```
```{r}
#| label: Simulation
bootstrap_p(m_m, no_of_sims = 10, threads = 4)
```


## Power law followers' distribution
```{r}
m_m = displ$new(users_aggregate[n_followers>0]$n_followers)
est_xmin = estimate_xmin(m_m)
m_m$setXmin(est_xmin)
est_pars = estimate_pars(m_m)
m_m$setPars(est_pars)
```

```{r}
plot(m_m)
lines(m_m, col = 2)
```
```{r}
#| label: Simulation
bootstrap_p(m_m, no_of_sims = 10, threads = 4)
```

## Social popularity

```{r}
ggplot(data = users_aggregate, aes(social_popularity)) + geom_density() + scale_x_log10()
```


# Network analysis

## Network representation: users connected by follow
```{r}
users_repos
users_on_users = merge.data.table(
  x = users_repos,
  y = users_repos,
  by = 'repo_id',
  allow.cartesian = T
)

v= c('author_id.x', 'author_id.y')
users_on_users
users = users_on_users[author_id.x != author_id.y, ..v]
users
users_degree_repos = users[, .N, by = author_id.x]
users_degree_repos
df = data.frame(x = users_degree_repos[, N])
ggplot(df, aes(x)) + geom_density() + scale_x_continuous(trans = 'log10')
followers_clean_no_na = remove_missing(followers_clean, na.rm = T)
```


```{r}
edges = data.frame("from" = followers_clean_no_na$follower_id, "to" = followers_clean_no_na$followee_id, stringsAsFactors = F )
g <- graph.data.frame(edges, directed = T)
g_simplified = igraph::simplify(
  g, 
  remove.multiple = T,
  remove.loops = F,
  edge.attr.comb = list(weight="sum", type="ignore")
  )
V(g)
g_simplified
```

```{r}
#| label: Adding commits to the graph

commits <- append("id", users_with_scores$n_commits)

names(commits) = commits
commits

# Loop through the attribute dataframe to add an attribute at the time, using the set_vertex_attribute function. 

for (i in 2:length(commits)){

  # Select attributes
  temp_attribute <- commits[,c(1,i)]

  # Match the attribute id with the id of the graph vertexes
  sorted_attr<- temp_attribute[match(temp_attribute[,1], V(g)$commits),2]
  
  # Set attributes
  g <- set_vertex_attr(g, name = (names(commits)[i]), 
                  index = V(g), value = sorted_attr)

}
```


```{r}
adjacency_matrix = get.adjacency(g)
adjacency_matrix
E(g_simplified)$weight
transitivity(g)
```
```{r}
average.path.length(g)
mean_distance(g)
g_degree = degree(g, loops = F, mode = 'in')

g_degree_table = setDT(data.frame(g_degree))
g_degree_table
g_degree_table = g_degree_table[, .N, by = g_degree]
g_degree_table[order(g_degree)]

ggplot(data = g_degree_table, aes(x = g_degree, y = N)) + geom_point() + scale_x_log10() + scale_y_log10()

degree_plot = ggplot(g_degree_table[g_degree > 0], aes(x = g_degree, y = N, colour = N)) + geom_point() +
  scale_x_log10(limits = c(1, NA)) + scale_y_log10() +
  labs(title = "In-degree of the followers network",
       x = "Number of followers",
       y = "Frequency"
       ) +
  scale_color_gradient(low =  '#44ac9e', high = '#ee7027', guide = 'none', trans = 'log') +
  theme(axis.line = element_line(arrow = arrow(angle = 20, length = unit(.10,"inches"),type = "closed")),
      axis.title.x = element_text(face = 'plain', size = 40),
      axis.title.y = element_text(face = 'plain', size = 40),
     text = element_text('Raleway', face = 'bold', size = 50),
     panel.background = element_rect(fill = "gray92", colour = NA),
     #title.text = element_text('NewCenturySchoolBook')
  )
```
```{r}
ggsave(
  '/home/pachy/Desktop/ACSAI/interust/graphs/degreeplot.png',
  plot = degree_plot,
  device = NULL,
  path = NULL,
  scale = 1,
  width = 2328,
  height = 1437,
  units = c("px"),
  dpi = 300,
  limitsize = TRUE,
  bg = NULL,
  create.dir = FALSE
)
```


Let's try a quick test of the power law on this distribution

```{r}
m_m = conpl$new(degree_distribution(g)[which(degree_distribution(g) > 0)])
est_xmin = estimate_xmin(m_m)
est_xmin
m_m$setXmin(est_xmin)
est_pars = estimate_pars(m_m)
m_m$setPars(est_pars)
```

```{r}
plot(m_m)
lines(m_m, col = 2)
```

```{r}
#| label: Simulation
bootstrap_p(m_m, no_of_sims = 20, threads = 4)
```
```{r}
#| label: Density

edge_density(g, loops = F)
```

Let's

```{r}
#| label: Clustering coefficient

transitivity(g, type = ')
```

```{r}
#| label: Diameter

diameter(g, directed=T)
```

```{r}
#| label: Centralization

centr_degree(g)$centralization
```

## Community detection

```{r}
#| label: Cliques
cliques_of_followers = cliques(g)
```

```{r}
#| label: Cluster edge betwenness

ceb <- coreness(g)
ceb
colrs <- adjustcolor( c("gray50", "tomato", "gold", "yellowgreen"), alpha=.6)
tmp = data.frame(x = ceb)
ceb_indexed = data.frame(x = as.numeric(rownames(tmp)), y = tmp[, 1])
ceb_indexed
users_aggregate
users_aggregate_with_clusters = merge.data.table(
  x = users_aggregate,
  y = ceb_indexed,
  by.x = 'id',
  by.y = 'x'
)
setnames(users_aggregate_with_clusters, 'y', 'cluster')
users_aggregate[n_commits != 0]
users_aggregate_with_clusters
aggregate_clusters = users_aggregate_with_clusters[, mean(n_stars_received), by = 'cluster']
aggregate_clusters
ggplot(data = aggregate_clusters, aes(V1)) + geom_density() + scale_x_l
plot(, vertex.size=ceb*6, vertex.label=ceb, vertex.color=colrs[ceb])
```

```{r}
assortativity_degree(g)
```


# Influencers disjoint from hard workers?

## Analysis on followers

```{r}
followers_clean_no_na = remove_missing(followers_clean, na.rm = T)
edges = data.frame("from" = followers_clean_no_na$follower_id, "to" = followers_clean_no_na$followee_id, stringsAsFactors = F )
g <- graph.data.frame(edges, directed = T)
V(g)$name = as.numeric(V(g)$name)
```

```{r}
#| label: Inserting the cluster attribute

users_aggregate_cluster[id %in% V(g)$name]

tmp = setDT(data.frame(V(g)$name))
setnames(tmp, 'V.g..name', 'id')
tmp$index = 1:nrow(tmp)
tmp

users_aggregate_cluster_ingraph = merge.data.table(
  x = tmp,
  y = users_aggregate_cluster,
  by = 'id',
  all.x = T
)

users_aggregate_cluster_ingraph = users_aggregate_cluster_ingraph[is.na(cluster), cluster := 0]
users_aggregate_cluster_ingraph[cluster != 0]
users_aggregate_cluster_ingraph = users_aggregate_cluster_ingraph[order(users_aggregate_cluster_ingraph$index)]
users_aggregate_cluster_ingraph
V(g)

# Set attributes
g <- set_vertex_attr(g, name = 'cluster', value = users_aggregate_cluster_ingraph$cluster)
V(g)$cluster
g_extremes = delete.vertices(g, V(g)[cluster == 0])
g_extremes = delete.vertices(g, V(g)[cluster == 2 | cluster == 3])
length(g)
V(g)
```
```{r}
modularity(g_extremes, V(g_extremes)$cluster)
```
```{r}
mean_distance(g_extremes, directed = T, unconnected = T)
```

```{r}
#| label: Plot
length(V(g))

g_sample = delete.vertices(g, V(g)[sample(length(V(g)), 9300)])
length(V(g_sample))
colrs <- adjustcolor( c("gray50", "tomato", "gold", "yellowgreen"), alpha=.6)
plot(g_sample, vertex.color=colrs[V(g_sample)$cluster], vertex.label = '')
```
```{r}
transitivity(g)
```

## Analysis on repositories

```{r}
users_on_users = merge.data.table(
  x = users_repos,
  y = users_repos,
  by = 'repo_id',
  allow.cartesian = T
)

v= c('author_id.x', 'author_id.y')
users_on_users
users_directed = users_on_users[author_id.x != author_id.y, ..v]
```

```{r}
edges = data.frame("from" = users_directed$author_id.x, "to" = users_directed$author_id.y, stringsAsFactors = F )
g <- graph.data.frame(edges, directed = F)
V(g)$name = as.numeric(V(g)$name)
V(g)
```

```{r}
#| label: Inserting the cluster attribute

users_aggregate_cluster[id %in% V(g)$name]

tmp = setDT(data.frame(V(g)$name))
tmp
setnames(tmp, 'V.g..name', 'id')
tmp$index = 1:nrow(tmp)
tmp

users_aggregate_cluster_ingraph = merge.data.table(
  x = tmp,
  y = users_aggregate_cluster,
  by = 'id',
  all.x = T
)

users_aggregate_cluster_ingraph = users_aggregate_cluster_ingraph[is.na(cluster), cluster := 0]
users_aggregate_cluster_ingraph[cluster != 0]
users_aggregate_cluster_ingraph = users_aggregate_cluster_ingraph[order(users_aggregate_cluster_ingraph$index)]
users_aggregate_cluster_ingraph
V(g)

# Set attributes
g <- set_vertex_attr(g, name = 'cluster', value = users_aggregate_cluster_ingraph$cluster)
V(g)$cluster
g = delete.vertices(g, V(g)[cluster == 0])
g_extremes = delete.vertices(g, V(g)[cluster == 2 | cluster == 3])
g
```

```{r}
modularity(g_extremes, V(g_extremes)$cluster)
```

```{r}
#| label: Plot
length(V(g))

g_sample = delete.vertices(g, V(g)[sample(length(V(g)), 9300)])
length(V(g_sample))
colrs <- adjustcolor( c("gray50", "tomato", "gold", "yellowgreen"), alpha=.6)
plot(g_sample, vertex.color=colrs[V(g_sample)$cluster], vertex.label = '')
```
```{r}
transitivity(g)
```


# Relationship between commits and stars

```{r}
usage_sample = setDT(read.csv('/home/pachy/Desktop/ACSAI/interust/data/sample/examples/preprocessed/usage.csv'))
contributions = setDT(read.csv('/home/pachy/Desktop/ACSAI/interust/data/sample/examples/preprocessed/contributions.csv'))

usage_sample = usage_sample[, timestamp := anytime(timestamp)]
diff(usage_sample$stars, lag=1)
contributions = contributions[, timestamp := anytime(timestamp)]
```
Let's see how do the number of stars and commits relate to each other

```{r}
usage_sample
ggplot(usage_sample, aes(commits, stars)) + geom_point()
```
Let's try to add a stars delta instead of a star cumulative count:
```{r}
diff(usage_sample$stars, lag=1)
usage_sample$stars_diff = c(NA, diff(usage_sample$stars, lag=1))
usage_sample[, ':='(repo_name = NULL,
                    commits_cumul = NULL,
                    active_developers = NULL)]
usage_sample[1]$stars_diff = 0

for (i in 1:(nrow(usage_sample) - 1)){
  if (usage_sample[i]$repo_id != usage_sample[i+1]$repo_id){
    usage_sample[i+1]$stars_diff = 0
  }
}
```

We can now create the same cumulative count with months, instead of stars:

```{r}
usage_sample
as.numeric(month(usage_sample[3]$timestamp) - month(usage_sample[1]$timestamp))

usage_sample$timestamp_diff = 1:nrow(usage_sample)
months_passed = 1
usage_sample[1]$timestamp_diff = 0

for (i in 2:nrow(usage_sample)){
  if (usage_sample[i-1]$repo_id != usage_sample[i]$repo_id){
    months_passed = 1
    usage_sample[i]$timestamp_diff = 0
  } else {
    usage_sample[i]$timestamp_diff = months_passed 
    months_passed = months_passed + 1
  }
}

usage_sample
```

Let's do a preliminary analysis. We can aggregate on the time_stamp and do an analysis on commits vs stars_diff

```{r}
usage_sample_by_timepassed = usage_sample[, .(stars = mean(stars_diff), commits = mean(commits)), by = timestamp_diff]

usage_sample_by_timepassed = usage_sample_by_timepassed[, ratio := stars/commits]
usage_sample_by_timepassed
stars_plot = ggplot(usage_sample_by_timepassed, aes(timestamp_diff, ratio, colour = timestamp_diff)) + geom_point() +
  labs(title = "Ratio of stars over commits at the passing of time",
       x = "Age of the repository in months",
       y = TeX(r"($\frac{stars}{commits}$)")
       ) +
  scale_color_gradient(low =  '#44ac9e', high = '#ee7027', guide = 'none') +
  theme(axis.line = element_line(arrow = arrow(angle = 20, length = unit(.10,"inches"),type = "closed")),
      axis.title.x = element_text(face = 'plain', size = 40),
      axis.title.y = element_text(face = 'plain', size = 40),
     text = element_text('Raleway', face = 'bold', size = 50),
     panel.background = element_rect(fill = "gray92", colour = NA),
     #title.text = element_text('NewCenturySchoolBook')
  )
ggsave(
  '/home/pachy/Desktop/ACSAI/interust/graphs/starsplot.png',
  plot = stars_plot,
  device = NULL,
  path = NULL,
  scale = 1,
  width = 2328,
  height = 1437,
  units = c("px"),
  dpi = 300,
  limitsize = TRUE,
  bg = NULL,
  create.dir = FALSE
)
```
We can even try do it time-step by time-step

```{r}
ggplot(usage_sample[timestamp_diff == 10], aes(stars_diff, commits)) + geom_point() + scale_x_log10() + scale_y_log10()
```


```{r}
contributions = contributions[order(user_id, repo_id, timestamp)]
contributions
```

Let's try to do the same with commits here

```{r}
contributions = contributions[, repo_name := NULL]
contributions
contributions$commit_diff = 1:nrow(contributions)

col_contributions = c('repo_id', 'user_id')
contributions[1]$commit_diff = contributions[1]$nb_commits
!identical(contributions[4, ..col_contributions], contributions[5, ..col_contributions])

for (i in 2:nrow(contributions)){
  if (!identical(contributions[i-1, ..col_contributions], contributions[i, ..col_contributions])) {
    contributions[i]$commit_diff = contributions[i]$nb_commits
  } else {
    contributions[i]$commit_diff = contributions[i]$nb_commits - contributions[i - 1]$nb_commits 
  }
}

contributions = contributions[commit_diff>=0]
contributions[commit_diff>=0]
```

We can now create the same cumulative count with months:
```{r}
#| label: Elapsed month function
elapsed_months <- function(end_date, start_date) {
    ed <- as.POSIXlt(end_date)
    sd <- as.POSIXlt(start_date)
    12 * (ed$year - sd$year) + (ed$mon - sd$mon)
}
elapsed_months(contributions[13]$timestamp, contributions[12]$timestamp)
```

```{r}
contributions$timestamp_diff = 1:nrow(contributions)
contributions[1]$timestamp_diff = 0
begin = contributions[1]$timestamp

for (i in 2:nrow(contributions)){
  if (!identical(contributions[i-1, ..col_contributions], contributions[i, ..col_contributions])){
    contributions[i]$timestamp_diff = 0
    begin = contributions[i]$timestamp
  } else {
    contributions[i]$timestamp_diff = elapsed_months(contributions[i]$timestamp, begin)
  }
}

contributions = contributions[, commits := NULL]
contributions
```

Let's do a preliminary graph, seeing how many commits one does in timeseries with belonging to a repository:
```{r}
contributions_sample_by_timepassed = contributions[, .(commits = mean(commit_diff)), by = timestamp_diff]

contributions_sample_by_timepassed
ggplot(contributions_sample_by_timepassed, aes(timestamp_diff, commits)) + geom_point()
```

Let's see, instead, how does the star ratio vary with time:

```{r}
contributions
usage_sample

contributions_with_stars = merge.data.table(
  x = contributions,
  y = usage_sample,
  by = c('repo_id', 'timestamp')
)

contributions_with_stars = contributions_with_stars[, timestamp_diff.y := NULL]
setnames(contributions_with_stars, 'timestamp_diff.x', 'timestamp_diff')
contributions_with_stars
```

```{r}
contributions_ratio_by_timepassed = contributions_with_stars[, .(commits = mean(commit_diff), stars = mean(stars_diff), downloads = mean(downloads)), by = timestamp_diff]

contributions_ratio_by_timepassed = contributions_ratio_by_timepassed[, ':='(ratio_stars = stars/commits, ratio_downloads = downloads/commits)]
contributions_ratio_by_timepassed
ggplot(contributions_ratio_by_timepassed, aes(x = timestamp_diff)) + geom_point(aes(y = ratio_stars, colour = 'red')) + geom_point(aes(y = ratio_downloads, colour = 'blue')) + scale_y_log10()  + scale_color_manual(
    labels = c("Download ratio", "Stars ratio"),
    values = c("blue", "red")
    )
```

And now, let's try to do it time-step by time-step

```{r}
ggplot(contributions_with_stars[timestamp_diff == 7], aes(commits, stars_diff)) + geom_point() + scale_x_log10() + scale_y_log10()
```

# Sponsorships and distance from top users
## Cleaning

Let's see what's inside the sponsorship table.

```{r}
sponsors_user

sponsors_user_clean = sponsors_user[, created_at := anytime(created_at)]
sponsors_user_clean

#First sponsorship


```

Now, we can select all the users that have been sponsored at least once

```{r}
sponsored_users = unique(sponsors_user[, 'sponsored_id'])

sponsored_users
```

Let's now check for the intersection between vertices and sponsored_users

```{r}
vertices = setDT(list(V(g)$name))

setnames(vertices, 'V1', 'id')

sponsored_vertices = merge.data.table(
  x = vertices,
  y = sponsored_users,
  by.x = 'id',
  by.y = 'sponsored_id'
)

sponsored_vertices
V(g)[name %in% unlist(sponsored_vertices)]
```

We can do a demo of distances to understand how it actually works.

```{r}
graph <- make_ring(10)
d = distances(graph)
d
d_min = apply(d, 1, FUN = max)
d_min
shortest_paths(graph, 5)
all_shortest_paths(graph, 1, 6:8)
mean_distance(graph)
## Weighted shortest paths
el <- matrix(ncol=3, byrow=TRUE,
             c(1,2,0, 1,3,2, 1,4,1, 2,3,0, 2,5,5, 2,6,2, 3,2,1, 3,4,1,
               3,7,1, 4,3,0, 4,7,2, 5,6,2, 5,8,8, 6,3,2, 6,7,1, 6,9,1,
               6,10,3, 8,6,1, 8,9,1, 9,10,4) )
graph2 <- add_edges(make_empty_graph(10), t(el[,1:2]), weight=el[,3])
graph2
distances(graph2, mode="out")
```

## To top users

Let's see the distance:

```{r}
distance_sponsored_topg = distances(graph = g,
               v = V(g)[name %in% unlist(sponsored_vertices)],
               to = V(g)[cluster == 4],
               mode = 'out'
               )

rownames(distance_sponsored_topg)

distance_st_min = apply(distance_sponsored_topg, 1, FUN = min)
distance_st_min = setDT(data.frame(distance_st_min))
setnames(distance_st_min, 'distance_st_min', 'distance')
colnames(distance_st_min)
nrow(distance_st_min[distance == Inf])
distance_st_min[distance == Inf]$distance = 7
```


And now, we can actually plot it

```{r}
ggplot(distance_st_min, aes(distance)) + geom_histogram()
```

## From top users

Let's do everything in the opposite order:

```{r}
distance_sponsored_topg_reversed = distances(graph = g,
               v = V(g)[name %in% unlist(sponsored_vertices)],
               to = V(g)[cluster == 4],
               mode = 'in'
               )

rownames(distance_sponsored_topg)
distance_sponsored_topg_reversed[is.infinite(distance_sponsored_topg_reversed)] = NA
distance_st_min_rev = apply(distance_sponsored_topg_reversed, 1, FUN = min, na.rm= T)
distance_st_min_rev
distance_st_min_rev = setDT(data.frame(distance_st_min_rev))
distance_st_min_rev
setnames(distance_st_min_rev, 'distance_st_min_rev', 'distance')
colnames(distance_st_min)
nrow(distance_st_min_rev[distance == Inf])
#distance_st_min_rev[distance == Inf]$distance = 6
```


And now, we can actually plot it

```{r}
ggplot(distance_st_min_rev, aes(distance)) + geom_histogram()
```


But let's check if the distance between the nodes is mostly one:

```{r}
all_distances = mean_distance(g, directed = T, unconnected = T)
all_distances
```

```{r}
distance_all_topg_reversed = distances(graph = g,
               to = V(g)[cluster == 1],
               mode = 'in'
               )

rownames(distance_sponsored_topg)
distance_all_topg_reversed[is.infinite(distance_all_topg_reversed)] = 10
distance_at_min_rev = apply(distance_all_topg_reversed, 1, FUN = min, na.rm= T)
distance_st_min_rev
distance_at_min_rev = setDT(data.frame(distance_at_min_rev))
distance_st_min_rev
setnames(distance_at_min_rev, 'distance_at_min_rev', 'distance')
colnames(distance_st_min)
nrow(distance_st_min_rev[distance == Inf])
#distance_st_min_rev[distance == Inf]$distance = 6
```

```{r}
ggplot(distance_at_min_rev, aes(distance)) + geom_histogram()
```
WOW!!!

## From influencers

We can try to check how the situation goes with influencers:

```{r}
distance_sponsored_influencers_reversed = distances(graph = g,
               v = V(g)[name %in% unlist(sponsored_vertices)],
               to = V(g)[cluster == 1],
               mode = 'in'
               )

rownames(distance_sponsored_topg)
distance_sponsored_influencers_reversed
distance_sponsored_influencers_reversed[is.infinite(distance_sponsored_influencers_reversed)] = NA
distance_si_min_rev = apply(distance_sponsored_influencers_reversed, 1, FUN = min, na.rm = T)
distance_si_min_rev
distance_si_min_rev = setDT(data.frame(distance_si_min_rev))
distance_si_min_rev
setnames(distance_si_min_rev, 'distance_si_min_rev', 'distance')
colnames(distance_si_min_rev)
nrow(distance_si_min_rev[distance == Inf])
distance_si_min_rev[distance == Inf]$distance = 6
```

```{r}
ggplot(distance_si_min_rev, aes(distance)) + geom_histogram(aes(y = ..count../sum(..count..)))
```

#A bit of exploration on the graph
## Are the groups following reciprocals?

```{r}
g_influencers = delete_vertices(g, V(g)[cluster != 1])
g_topg = delete_vertices(g, V(g)[cluster != 4])
```


```{r}
reciprocity_influencers = reciprocity(g_influencers, mode = 'ratio')
reciprocity_topg = reciprocity(g_topg, mode = 'ratio')
reciprocity_total = reciprocity(g, mode = 'ratio')

reciprocity_total
```

```{r}
transitivity(g_influencers, type = 'global')
triad_census(g_influencers)
closeness(g_influencers, mode = 'out', normalized = T)$closeness
```

```{r}
transitivity(g_topg, type = 'global')
triad_census(g_topg)
```

```{r}
transitivity(g, type = 'global')
```

```{r}
topg_betweenness = betweenness(g, v = V(g)[V(g)$cluster == 4])
topg_betweenness
topg_betweenness = setDT(data.frame(topg_betweenness))
topg_betweenness
mean(topg_betweenness$topg_betweenness)
```

```{r}
influencers_betweenness = betweenness(g, v = V(g)[V(g)$cluster == 1])
influencers_betweenness
influencers_betweenness = setDT(data.frame(influencers_betweenness))
influencers_betweenness
mean(influencers_betweenness$influencers_betweenness)
```

```{r}
useless_betweenness = betweenness(g, v = V(g)[V(g)$cluster == 2 | V(g)$cluster == 3])
useless_betweenness = setDT(data.frame(useless_betweenness))
mean(useless_betweenness$useless_betweenness)
```

## Hub and authority score

```{r}
hs = hub_score(g)
list(hs$vector)
hs_clean = data.frame(hs$vector)
hs_clean$id = as.numeric(rownames(hs_clean))
users_network_info = merge.data.table(
  x = users_aggregate_cluster,
  y = hs_clean,
  by = 'id'
)

users_network_info[, c('hs.vector')]
setnames(users_network_info, 'hs.vector', 'hub_score')
```

We can now plot the distribution of the hubs scores: to which groups do they belong?

```{r}
users_network_info = users_network_info[order(-users_network_info$hub_score)]
ggplot(users_network_info[1:100, ], aes(cluster)) +
  geom_histogram(theme()) +
  ylab('Hub score') + 
  scale_x_continuous(name ="Type of user",
                     labels=c('1' = "Influencers", '2' = "Ants", '3'= "Small users", '4'= "Top users"))
```

```{r}
as = authority_score(g)
as_clean = data.frame(as$vector)
as_clean$id = as.numeric(rownames(as_clean))
users_network_info = merge.data.table(
  x = users_network_info,
  y = as_clean,
  by = 'id'
)

setnames(users_network_info, 'as.vector', 'authority_score')
users_network_info = users_network_info[order(-users_network_info$authority_score)]

users_network_info[, c('authority_score')]
```

We can now plot the distribution of the hubs scores: to which groups do they belong?

```{r}
users_network_info = users_network_info[order(-users_network_info$authority_score)]
ggplot(users_network_info[1:500, ], aes(cluster)) +
  geom_histogram(aes(colour = cluster)) +
  ylab('Top 500 per authority score') + 
  scale_x_continuous(name ="Type of user",
                     labels=c('1' = "Influencers", '2' = "Ants", '3'= "Small users", '4'= "Top users"))
```

```{r}
out_degree =  users_aggregate_cluster[, mean(n_following), by = cluster]
out_degree
```
```{r}
distance_inf_topg = distances(graph = g,
               v = V(g)[V(g)$cluster == 1],
               to = V(g)[cluster == 4],
               mode = 'out'
               )
distance_inf_topg = data.frame(distance_inf_topg)
distance_inf_topg
distance_inf_topg[is.infinite(distance_inf_topg)] = NA
distance_inf_topg
mean_distance_inf_topg =mean(distance_inf_topg, rm.na = T)

distance_topg_inf = distances(graph = g,
               v = V(g)[V(g)$cluster == 1],
               to = V(g)[cluster == 4],
               mode = 'in'
               )
distance_topg_inf =mean(distance_topg_inf)
mean_distance_inf_topg
```

# Is followers a sublinear BA model?

First of all, we want to determine the characteristic of our model.

```{r}
g_label_clustered = delete_vertices(g, V(g)[V(g)$cluster == 0])
n = vcount(g)
m = ecount(g)
```

Now we can create the BA model:

```{r}
g_ba_theoretical = sample_pa(n = n,
                             power = 0.5
                             )
```

And now we can confront the two.

```{r}
gm(g_label_clustered, g_ba_theoretical)
```


# Model for predicting followers
## GBM

```{r}
colnames(users_aggregate)
users_aggregate_model = copy(users_with_scores)
users_aggregate_model = users_aggregate_model[, ':='(id = NULL,
                                               user_id = NULL,
                                               login = NULL,
                                               created_at = NULL,
                                               attributes = NULL,
                                               is_bot.x = NULL,
                                               creation_identity = NULL,
                                               working_activity = NULL,
                                               social_popularity = NULL,
                                               social_activity = NULL)]

colnames(users_aggregate_model)
model_trained = gbm(formula = n_followers ~ .,
        data = users_aggregate_model)


summary(model_trained, order = T, plotit = F)
print(model_trained)
```

Let's try to see some metrics about this tree

```{r}
relative.influence(model_trained, scale = T, sort = T)

plot(x = model_trained,
     i.var = c('n_following'),
     level.plot = T
    )
```

Other tests

```{r}
gbm.perf(object = model_trained)
```
```{r}
permutation.test.gbm(model_trained, n.trees = 100)
```


## Model testing

```{r}
set.seed(42)
intrain <- createDataPartition(y=users_aggregate_model$n_followers, p=0.7, list=FALSE)
train = users_aggregate_model[intrain]
test = users_aggregate_model[-intrain]
```


```{r}
model_trained = gbm(formula = n_followers ~ .,
                    data = train)
best_iter = gbm.perf(model_trained)
predictions = predict(model_trained, test, n.trees = best_iter)
ss_total = sum((test$n_followers - mean(test$n_followers))^2)
ss_residual = sum((test$n_followers - predictions)^2)
r_squared = 1 - ss_residual/ss_total

r_squared
```


```{r}
gbm.roc.area(obs = test$n_followers,
             pred = predictions)
```

## Ranger

```{r}
model_ranger = ranger(formula = n_followers ~ .,
                    data = train)
model_ranger
```

## Perceptron

```{r}
h2o.init()
train_frame = as.h2o(train)
test_frame = as.h2o(test)

model_perceptron = h2o::h2o.deeplearning(y = 'n_followers',
                                         training_frame = train_frame,
                                         validation_frame = test_frame,
                                         hidden  = c(200, 200, 200),
                                         epochs = 30,
                                         activation = 'Rectifier',
                                         rate = 0.005)
perf = h2o.performance(model_perceptron, test_frame)
perf
h2o.r2(model_perceptron, valid = T)
h2o.accuracy(perf)
```

## Random forest

```{r}
train
train_frame = as.h2o(train)
model_xgboost = h2o.randomForest(y = 'n_followers',
                         training_frame = train_frame
                         )

h2o.r2(model_xgboost)
```

## Simple regression

```{r}
train
linear_model = lm(n_followers ~ .,
   data = train)
summary(linear_model)
```

```{r}
test[n_commits > 0]
```

```{r}
users_aggregate[n_commits>0]
```

```{r}
users_with_scores[n_stars_received>0]
```

# Network defined on repositories

```{r}
contributions_raw = read.csv('/home/pachy/Desktop/ACSAI/interust/data/sample/examples/preprocessed/contributions.csv')

contributions_raw = unique(contributions_raw[, c('repo_id', 'user_id')])
contributions_raw
```

We can now aggregate users if they work on the same repositories

```{r}
users_on_repos = merge.data.table(
  x = setDT(contributions_raw),
  y = setDT(contributions_raw),
  by = 'repo_id',
  allow.cartesian = T
)

users_on_repos = users_on_repos[user_id.x != user_id.y]
users_on_repos
```

```{r}
edges = data.frame("from" = users_on_repos$user_id.x, "to" = users_on_repos$user_id.y, stringsAsFactors = F )
g_ur <- graph.data.frame(edges, directed = F)
```

```{r}
degree_users_on_repos = degree(g_ur, loops = F)
degree_users_on_repos = setDT(data.frame(degree_users_on_repos))
degree_distr_ur = degree_users_on_repos[, .N, by = degree_users_on_repos]
degree_distr_ur[order(degree_users_on_repos)]
ggplot(degree_distr_ur, aes(x = degree_users_on_repos, y = N)) +
  geom_point() +
  scale_y_log10() + 
  scale_x_log10()

```










